{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Reestimating Preference Parameters Using Experimental Data\n",
    "\n",
    "This notebook allows you to reestimate preference parameters using the full experimental data for each subject.\n",
    "\n",
    "You can use the original prior distribution and likelihood specification, or you can specify a new prior or choice model that you would like to estimate.\n",
    "\n",
    "Begin by importing the required packages and BACE functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "import os, sys, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "def import_parents(level=1):\n",
    "    global __package__\n",
    "    file = Path(os.path.abspath('')).resolve()\n",
    "    parent, top = file.parent, file.parents[level - 1]\n",
    "\n",
    "    sys.path.append(str(top))\n",
    "    try:\n",
    "        sys.path.remove(str(parent))\n",
    "    except ValueError: # already removed\n",
    "        pass\n",
    "\n",
    "    __package__ = '.'.join(parent.parts[len(top.parts):])\n",
    "    importlib.import_module(__package__) # won't be needed after that\n",
    "\n",
    "import_parents(level=2)\n",
    "\n",
    "# Import database connection and pmc function\n",
    "from app.database.db import table, decimal_to_float\n",
    "from app.bace.pmc_inference import pmc\n",
    "from app.bace.user_config import answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reestimate preference parameters using the existing BACE specifications, you can run the following line:\n",
    "\n",
    "```\n",
    "from bace.user_config import theta_params, likelihood_pdf, size_thetas\n",
    "```\n",
    "\n",
    "Alternatively, you can specify each of the following parameters using the same process that you used to define the components in `app/bace/user_config.py`:\n",
    "- `theta_params`: Specifies the prior distribution.\n",
    "- `likelihood_pdf`: Specify the likelihood of observing each answer in answers.\n",
    "- `size_thetas`: The size of the sample drawn from `theta_params`. Since speed is less important outside of an experiment, you can improve the precision of estimates by increasing this number.\n",
    "\n",
    "Note that the choice model defined in `likelihood_pdf` can depend on new preference parameters in `theta_params` or combinations of the original design components that an individual saw.\n",
    "\n",
    "In this example, we will use the existing BACE specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.bace.user_config import theta_params, likelihood_pdf, size_thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, specify `estimation_version` with notes that you want to record in the output file, and specify `output_file` with the path that you want to use to save your `.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Output file path.\n",
    "output_file = \"all.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to clean the design and answer histories for each individual in the database.\n",
    "\n",
    "It ensures that the design and answer histories are the same length for re-estimation; these can differ if, for example, an individual exited the survey early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_designs_and_answers(item, answers):\n",
    "\n",
    "    str_answers = [str(answer) for answer in answers]\n",
    "\n",
    "    design_hist = item['design_history'].copy()\n",
    "    answer_hist = item['answer_history'].copy()\n",
    "    answer_hist.extend([None] * (len(design_hist) - len(answer_hist)))\n",
    "\n",
    "    # Store Output\n",
    "    design_history = []\n",
    "    answer_history = []\n",
    "\n",
    "\n",
    "    for design, answer in zip(design_hist, answer_hist):\n",
    "\n",
    "        if (design is not None) and (answer is not None) and (str(answer) in str_answers):\n",
    "\n",
    "            design_history.append(design)\n",
    "            answer_history.append(answer)\n",
    "\n",
    "    return design_history, answer_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code queries your database for all items. Each item contains all information for an individual survey respondent, which is characterized by a unique `profile_id`.\n",
    "\n",
    "Note: You can also modify the code to get the data from a csv file after using `/data/save_data.py` instead of querying from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import openpyxl\n",
    "import csv\n",
    "\n",
    "id_column = 'profile_id' # Unique ID column for each profile\n",
    "table_name = 'bace-db' # Update this if the name of the database TableName in template.yaml is changed\n",
    "# Update `table_region` below to the region created with `sam deploy --guided`, saved in the SAM configuration file (samconfig.toml by default)\n",
    "#   if different from the default region in ~/.aws/config (or C:\\Users\\USERNAME\\.aws\\config)\n",
    "table_region = boto3.Session().region_name # example if different from default: table_region = 'us-east-2'\n",
    "# os.environ['AWS_PROFILE'] = \"YOUR_AWS_PROFILE_NAME\" # Set this if your current AWS login profile is not the default one -- see profiles in ~/.aws/config (or C:\\Users\\USERNAME\\.aws\\config)\n",
    "\n",
    "############################\n",
    "\n",
    "# Start database connection\n",
    "ddb = boto3.resource('dynamodb', region_name = table_region)\n",
    "table = ddb.Table(table_name)\n",
    "\n",
    "# Scan all data from DynamoDB table\n",
    "response = table.scan()\n",
    "db_items = response['Items']\n",
    "\n",
    "with open('profiles_to_test.csv', 'r', encoding='utf-8-sig') as file:\n",
    "    reader = csv.reader(file, delimiter='\\n')\n",
    "    profiles_to_test = [row[0] for row in reader]\n",
    "\n",
    "# Go beyond the 1mb limit: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html\n",
    "while 'LastEvaluatedKey' in response:\n",
    "    response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "    db_items.extend(response['Items'])\n",
    "\n",
    "db_items = [item for item in db_items if item['profile_id'] in profiles_to_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "likelihood_pdf() got an unexpected keyword argument 'design_a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m out \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m     estimates \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_posterior_estimates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     out\u001b[38;5;241m.\u001b[39mupdate(estimates)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;66;03m# Store output.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# You can add additional variables associated with an item using item.get('var') to the exported csv.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[67], line 33\u001b[0m, in \u001b[0;36mcompute_posterior_estimates\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     27\u001b[0m theta_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     28\u001b[0m     WTP\u001b[38;5;241m=\u001b[39mscipy\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, upper),\n\u001b[1;32m     29\u001b[0m     p\u001b[38;5;241m=\u001b[39mscipy\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39muniform()\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Compute posterior estimates using Population Monte Carlo\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m posterior_thetas \u001b[38;5;241m=\u001b[39m \u001b[43mpmc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtheta_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesign_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlikelihood_pdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize_thetas\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Calculate the mean and median posterior estimate for each parameter.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m estimates \u001b[38;5;241m=\u001b[39m posterior_thetas\u001b[38;5;241m.\u001b[39magg([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[0;32m~/Desktop/bace/app/bace/pmc_inference.py:24\u001b[0m, in \u001b[0;36mpmc\u001b[0;34m(theta_params, answer_history, design_history, likelihood_pdf, N, J)\u001b[0m\n\u001b[1;32m     19\u001b[0m w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(J):\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Compute importance weights\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     old_thetas, sampled_thetas, weights \u001b[38;5;241m=\u001b[39m \u001b[43mimportance_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_thetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesign_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood_pdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Store sampled points and associated weights\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     pool_thetas \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pool_thetas, sampled_thetas], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/bace/app/bace/pmc_inference.py:49\u001b[0m, in \u001b[0;36mimportance_sample\u001b[0;34m(old_thetas, theta_params, scale, answer_history, design_history, likelihood_pdf, N)\u001b[0m\n\u001b[1;32m     47\u001b[0m log_q \u001b[38;5;241m=\u001b[39m compute_q_logpdf(new_thetas, old_thetas, scale)\n\u001b[1;32m     48\u001b[0m log_prior \u001b[38;5;241m=\u001b[39m compute_prior_logpdf(new_thetas, theta_params)\n\u001b[0;32m---> 49\u001b[0m log_pi \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_lklhd_logpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_thetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesign_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood_pdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Calculate weights. Use of M is better for numerical stability.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m log_w \u001b[38;5;241m=\u001b[39m log_pi \u001b[38;5;241m+\u001b[39m log_prior \u001b[38;5;241m-\u001b[39m log_q\n",
      "File \u001b[0;32m~/Desktop/bace/app/bace/pmc_inference.py:92\u001b[0m, in \u001b[0;36mcompute_lklhd_logpdf\u001b[0;34m(thetas, answer_history, design_history, likelihood_pdf)\u001b[0m\n\u001b[1;32m     89\u001b[0m lklhd_logpdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ND):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Compute p(answer_i | thetas, design_i)\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     lklhd \u001b[38;5;241m=\u001b[39m \u001b[43mlikelihood_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswer_history\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdesign_history\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m         log_lklhd \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(lklhd)\n",
      "\u001b[0;31mTypeError\u001b[0m: likelihood_pdf() got an unexpected keyword argument 'design_a'"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "import numpy as np\n",
    "output = []\n",
    "\n",
    "for item in db_items:\n",
    "\n",
    "    # Conver DynamoDB Decimal type to floats\n",
    "    item = decimal_to_float(item)\n",
    "\n",
    "    # Get cleaned design and answer histories.\n",
    "    design_history, answer_history = clean_designs_and_answers(item, answers)\n",
    "    ND = len(design_history)\n",
    "\n",
    "    # Estimate preferences if the individual answered at least one question.\n",
    "    if ND > 0:\n",
    "\n",
    "        # try:\n",
    "\n",
    "            #################################################################################\n",
    "            ### Edit this block to update the method for calculating posterior estimates. ###\n",
    "            #################################################################################\n",
    "\n",
    "# Assuming pmc, answer_history, design_history, likelihood_pdf, size_thetas are already defined\n",
    "\n",
    "        def compute_posterior_estimates(i):\n",
    "            upper = float(i) / 10.0\n",
    "            theta_params = dict(\n",
    "                WTP=scipy.stats.uniform(0, upper),\n",
    "                p=scipy.stats.uniform()\n",
    "            )\n",
    "\n",
    "            # Compute posterior estimates using Population Monte Carlo\n",
    "            posterior_thetas = pmc(\n",
    "                theta_params,\n",
    "                answer_history,\n",
    "                design_history,\n",
    "                likelihood_pdf,\n",
    "                size_thetas\n",
    "            )\n",
    "\n",
    "            # Calculate the mean and median posterior estimate for each parameter.\n",
    "            estimates = posterior_thetas.agg(['mean', 'median', 'std']).to_dict()\n",
    "            estimates = {str(i) + '_' + key: value for key, value in estimates.items()}\n",
    "            return estimates\n",
    "\n",
    "        # Set a consistent random seed\n",
    "        np.random.seed(42)\n",
    "\n",
    "        out = {}\n",
    "        for i in range(1, 11):\n",
    "            estimates = compute_posterior_estimates(i)\n",
    "            out.update(estimates)\n",
    "\n",
    "                # Store output.\n",
    "        # You can add additional variables associated with an item using item.get('var') to the exported csv.\n",
    "        individual_output = {\n",
    "            \"profile_id\": item.get(\"profile_id\"),\n",
    "            \"n_designs\": ND,\n",
    "            \"param\": item.get(\"param\"),\n",
    "            **out,\n",
    "        }\n",
    "\n",
    "        output.append(individual_output)\n",
    "\n",
    "\n",
    "# Convert output to dataframe and write to .csv\n",
    "output_df = pd.json_normalize(output)\n",
    "output_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can update this code block above to adjust other components of the reestimation.\n",
    "\n",
    "For example, the notebook defaults to using Population Monte Carlo to reestimate preferences.\n",
    "\n",
    "If you want to estimate preferences using an alternative method, you can update the following block. For example, you could perform logistic regression using each individual's data or you can use an alternative method to perform Bayesian Inference.\n",
    "\n",
    "You can also edit what statistics are saved when forming estimates.\n",
    "\n",
    "```python\n",
    "#############################################################################\n",
    "### Edit this block to update the method for calculating posterior estimates.\n",
    "#############################################################################\n",
    "\n",
    "# Compute posterior estimates using Population Monte Carlo\n",
    "posterior_thetas = pmc(\n",
    "    theta_params, \n",
    "    answer_history, \n",
    "    design_history, \n",
    "    likelihood_pdf, \n",
    "    size_thetas\n",
    ")\n",
    "\n",
    "# Calculate the mean and median posterior estimate for each parameter.\n",
    "# Update this code to store alternative statistics.\n",
    "estimates = posterior_thetas.agg(['mean', 'median']).to_dict()\n",
    "\n",
    "#############################################################################\n",
    "#############################################################################\n",
    "```\n",
    "\n",
    "We hope this notebook is useful for recomputing preference estimates after an experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
